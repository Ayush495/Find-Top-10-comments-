{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = requests.get('https://www.reddit.com/r/AdviceAnimals/comments/5y54am/my_uncle_is_an_awesome_boss/')\n",
    "tree = html.fromstring(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = tree.xpath('//div[@class=\"md\"]/p/text()')\n",
    "#This will create a list of points \n",
    "points = tree.xpath('//span[@class=\"score unvoted\"]/@title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "int_points_array = [int(numeric_string) for numeric_string in points]\n",
    "a={'Comments':pd.Series(comments),'Points':pd.Series(int_points_array)}\n",
    "df=pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dell\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>This is a good fucking way to look at any poli...</td>\n",
       "      <td>6997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The customer isn't always right. I feel like t...</td>\n",
       "      <td>3981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Politicical views dont matter, country of orig...</td>\n",
       "      <td>3583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People like your uncle, the world needs more o...</td>\n",
       "      <td>2049.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>I would happily be your uncle's horse!</td>\n",
       "      <td>1720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If we forget about the politics of the situati...</td>\n",
       "      <td>1710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The saying</td>\n",
       "      <td>1356.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Your uncle is legend. Thank you and your family</td>\n",
       "      <td>933.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let's not forget the politics of the situation...</td>\n",
       "      <td>927.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>This is kinda the norm for immigrants, legal o...</td>\n",
       "      <td>895.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comments  Points\n",
       "17  This is a good fucking way to look at any poli...  6997.0\n",
       "19  The customer isn't always right. I feel like t...  3981.0\n",
       "18  Politicical views dont matter, country of orig...  3583.0\n",
       "0   People like your uncle, the world needs more o...  2049.0\n",
       "60             I would happily be your uncle's horse!  1720.0\n",
       "1   If we forget about the politics of the situati...  1710.0\n",
       "20                                        The saying   1356.0\n",
       "59    Your uncle is legend. Thank you and your family   933.0\n",
       "2   Let's not forget the politics of the situation...   927.0\n",
       "61  This is kinda the norm for immigrants, legal o...   895.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted=df.sort('Points',ascending=False)[0:10]\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'This is a good fucking way to look at any political situation these days, strip it down to its basic form, and analys it there...',\n",
       "       \"The customer isn't always right. I feel like the only people who say that are past 2005 are customers or shitty managers.\",\n",
       "       'Politicical views dont matter, country of origin does not matter, throw all of that shit away, and think of the basic scenario as u stated \"employe threatened by a custommer\"',\n",
       "       'People like your uncle, the world needs more of them. ',\n",
       "       \"I would happily be your uncle's horse!\",\n",
       "       \"If we forget about the politics of the situation, it's a customer threatening an employee. That's something that should not be tolerated. \",\n",
       "       'The saying ', 'Your uncle is legend. Thank you and your family',\n",
       "       \"Let's not forget the politics of the situation. I'm an immigrant just like your uncle's employee, and I appreciate employers that stand up against hate speech. \",\n",
       "       'This is kinda the norm for immigrants, legal or illegal. They appreciate the hell out of being in this country and work their ass off with zero bullshit. You wonder why employers love hiring immigrant workers.'], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_array=df_sorted['Comments'].values[0:10]\n",
    "comments_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect =CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_words=vect.fit_transform(comments_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Bag of words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2005</th>\n",
       "      <th>about</th>\n",
       "      <th>against</th>\n",
       "      <th>all</th>\n",
       "      <th>always</th>\n",
       "      <th>an</th>\n",
       "      <th>analys</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>appreciate</th>\n",
       "      <th>...</th>\n",
       "      <th>why</th>\n",
       "      <th>with</th>\n",
       "      <th>wonder</th>\n",
       "      <th>work</th>\n",
       "      <th>workers</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2005  about  against  all  always  an  analys  and  any  appreciate  ...   \\\n",
       "0     0      0        0    0       0   0       1    1    1           0  ...    \n",
       "1     1      0        0    0       1   0       0    0    0           0  ...    \n",
       "2     0      0        0    1       0   0       0    1    0           0  ...    \n",
       "3     0      0        0    0       0   0       0    0    0           0  ...    \n",
       "4     0      0        0    0       0   0       0    0    0           0  ...    \n",
       "5     0      1        0    0       0   1       0    0    0           0  ...    \n",
       "6     0      0        0    0       0   0       0    0    0           0  ...    \n",
       "7     0      0        0    0       0   0       0    1    0           0  ...    \n",
       "8     0      0        1    0       0   1       0    1    0           1  ...    \n",
       "9     0      0        0    0       0   0       0    1    0           1  ...    \n",
       "\n",
       "   why  with  wonder  work  workers  world  would  you  your  zero  \n",
       "0    0     0       0     0        0      0      0    0     0     0  \n",
       "1    0     0       0     0        0      0      0    0     0     0  \n",
       "2    0     0       0     0        0      0      0    0     0     0  \n",
       "3    0     0       0     0        0      1      0    0     1     0  \n",
       "4    0     0       0     0        0      0      1    0     1     0  \n",
       "5    0     0       0     0        0      0      0    0     0     0  \n",
       "6    0     0       0     0        0      0      0    0     0     0  \n",
       "7    0     0       0     0        0      0      0    1     2     0  \n",
       "8    0     0       0     0        0      0      0    0     1     0  \n",
       "9    1     1       1     1        1      0      0    1     0     1  \n",
       "\n",
       "[10 rows x 121 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(bag_words.toarray(),columns=vect.get_feature_names())\n",
    "#From this no conclusion can be drawn as words like (is,a,the will be repeated many times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TfIDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: to, TF-IDF: 0.12876\n",
      "\tWord: it, TF-IDF: 0.09632\n",
      "\tWord: these, TF-IDF: 0.06438\n",
      "\tWord: form, TF-IDF: 0.06438\n",
      "\tWord: good, TF-IDF: 0.06438\n",
      "\tWord: its, TF-IDF: 0.06438\n",
      "\tWord: there, TF-IDF: 0.06438\n",
      "\tWord: down, TF-IDF: 0.06438\n",
      "\tWord: any, TF-IDF: 0.06438\n",
      "\tWord: days, TF-IDF: 0.06438\n",
      "Top words in document 2\n",
      "\tWord: are, TF-IDF: 0.13995\n",
      "\tWord: The, TF-IDF: 0.10469\n",
      "\tWord: feel, TF-IDF: 0.06998\n",
      "\tWord: who, TF-IDF: 0.06998\n",
      "\tWord: n't, TF-IDF: 0.06998\n",
      "\tWord: customers, TF-IDF: 0.06998\n",
      "\tWord: 2005, TF-IDF: 0.06998\n",
      "\tWord: only, TF-IDF: 0.06998\n",
      "\tWord: people, TF-IDF: 0.06998\n",
      "\tWord: say, TF-IDF: 0.06998\n",
      "Top words in document 3\n",
      "\tWord: matter, TF-IDF: 0.1073\n",
      "\tWord: does, TF-IDF: 0.05365\n",
      "\tWord: employe, TF-IDF: 0.05365\n",
      "\tWord: as, TF-IDF: 0.05365\n",
      "\tWord: threatened, TF-IDF: 0.05365\n",
      "\tWord: throw, TF-IDF: 0.05365\n",
      "\tWord: Politicical, TF-IDF: 0.05365\n",
      "\tWord: think, TF-IDF: 0.05365\n",
      "\tWord: by, TF-IDF: 0.05365\n",
      "\tWord: custommer, TF-IDF: 0.05365\n",
      "Top words in document 4\n",
      "\tWord: world, TF-IDF: 0.16094\n",
      "\tWord: People, TF-IDF: 0.16094\n",
      "\tWord: needs, TF-IDF: 0.16094\n",
      "\tWord: them, TF-IDF: 0.16094\n",
      "\tWord: more, TF-IDF: 0.16094\n",
      "\tWord: like, TF-IDF: 0.09163\n",
      "\tWord: your, TF-IDF: 0.06931\n",
      "\tWord: uncle, TF-IDF: 0.06931\n",
      "\tWord: of, TF-IDF: 0.05108\n",
      "\tWord: the, TF-IDF: 0.03567\n",
      "Top words in document 5\n",
      "\tWord: horse, TF-IDF: 0.20118\n",
      "\tWord: would, TF-IDF: 0.20118\n",
      "\tWord: happily, TF-IDF: 0.20118\n",
      "\tWord: be, TF-IDF: 0.1505\n",
      "\tWord: 's, TF-IDF: 0.11454\n",
      "\tWord: I, TF-IDF: 0.11454\n",
      "\tWord: your, TF-IDF: 0.08664\n",
      "\tWord: uncle, TF-IDF: 0.08664\n",
      "Top words in document 6\n",
      "\tWord: That, TF-IDF: 0.13412\n",
      "\tWord: 's, TF-IDF: 0.07636\n",
      "\tWord: something, TF-IDF: 0.06706\n",
      "\tWord: we, TF-IDF: 0.06706\n",
      "\tWord: If, TF-IDF: 0.06706\n",
      "\tWord: about, TF-IDF: 0.06706\n",
      "\tWord: tolerated, TF-IDF: 0.06706\n",
      "\tWord: threatening, TF-IDF: 0.06706\n",
      "\tWord: should, TF-IDF: 0.06706\n",
      "\tWord: that, TF-IDF: 0.05776\n",
      "Top words in document 7\n",
      "\tWord: saying, TF-IDF: 0.80472\n",
      "\tWord: The, TF-IDF: 0.60199\n",
      "Top words in document 8\n",
      "\tWord: Your, TF-IDF: 0.35765\n",
      "\tWord: Thank, TF-IDF: 0.17883\n",
      "\tWord: legend, TF-IDF: 0.17883\n",
      "\tWord: family, TF-IDF: 0.17883\n",
      "\tWord: you, TF-IDF: 0.17883\n",
      "\tWord: your, TF-IDF: 0.15403\n",
      "\tWord: uncle, TF-IDF: 0.07702\n",
      "\tWord: is, TF-IDF: 0.07702\n",
      "\tWord: and, TF-IDF: 0.05676\n",
      "Top words in document 9\n",
      "\tWord: 's, TF-IDF: 0.06319\n",
      "\tWord: I, TF-IDF: 0.06319\n",
      "\tWord: just, TF-IDF: 0.0555\n",
      "\tWord: speech, TF-IDF: 0.0555\n",
      "\tWord: 'm, TF-IDF: 0.0555\n",
      "\tWord: stand, TF-IDF: 0.0555\n",
      "\tWord: up, TF-IDF: 0.0555\n",
      "\tWord: against, TF-IDF: 0.0555\n",
      "\tWord: Let, TF-IDF: 0.0555\n",
      "\tWord: hate, TF-IDF: 0.0555\n",
      "Top words in document 10\n",
      "\tWord: this, TF-IDF: 0.08941\n",
      "\tWord: This, TF-IDF: 0.06689\n",
      "\tWord: love, TF-IDF: 0.04471\n",
      "\tWord: out, TF-IDF: 0.04471\n",
      "\tWord: illegal, TF-IDF: 0.04471\n",
      "\tWord: for, TF-IDF: 0.04471\n",
      "\tWord: in, TF-IDF: 0.04471\n",
      "\tWord: why, TF-IDF: 0.04471\n",
      "\tWord: bullshit, TF-IDF: 0.04471\n",
      "\tWord: zero, TF-IDF: 0.04471\n"
     ]
    }
   ],
   "source": [
    "document1=tb(comments_array[0])\n",
    "document2=tb(comments_array[1])\n",
    "document3=tb(comments_array[2])\n",
    "document4=tb(comments_array[3])\n",
    "document5=tb(comments_array[4])\n",
    "document6=tb(comments_array[5])\n",
    "document7=tb(comments_array[6])\n",
    "document8=tb(comments_array[7])\n",
    "document9=tb(comments_array[8])\n",
    "document10=tb(comments_array[9])\n",
    "bloblist = [document1, document2, document3,document4,document5,document6,document7,document8,document9,document10]\n",
    "arr1=[]\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:10]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
    "        arr1.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the words ,removing stop words from it ,finding nouns in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize,Text,pos_tag\n",
    "arr2=[]\n",
    "def sentence_verification(sentence):\n",
    "    word_find=sentence\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(word_find)\n",
    "    filtered_sentence=[]\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "                     filtered_sentence.append(w)\n",
    "    #print (filtered_sentence)\n",
    "    text = Text(filtered_sentence)\n",
    "    tags = pos_tag(text)\n",
    "    #print (tags)\n",
    "    nouns = \"NN NNP NNS\".split()\n",
    "    verbs = \"VB VBD VBP VBG\".split()\n",
    "    for i in range(len(tags) -1):\n",
    "        if tags[i][1] in nouns:\n",
    "            print (tags[i][0],tags[i][1])\n",
    "            arr2.append(tags[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fucking NN\n",
      "way NN\n",
      "situation NN\n",
      "days NNS\n",
      "form NN\n",
      "analys NN\n",
      ".....................................................................................................\n",
      "customer NN\n",
      "people NNS\n",
      "customers NNS\n",
      "managers NNS\n",
      ".....................................................................................................\n",
      "views NNS\n",
      "matter NN\n",
      "country NN\n",
      "origin NN\n",
      "matter NN\n",
      "scenario NN\n",
      "u NN\n",
      "custommer NN\n",
      ".....................................................................................................\n",
      "People NNS\n",
      "uncle NN\n",
      "world NN\n",
      "needs NNS\n",
      ".....................................................................................................\n",
      "horse NN\n",
      ".....................................................................................................\n",
      "politics NNS\n",
      "situation NN\n",
      "customer NN\n",
      "threatening NN\n",
      "employee NN\n",
      "something NN\n",
      ".....................................................................................................\n",
      ".....................................................................................................\n",
      "uncle NN\n",
      "legend NN\n",
      "Thank NNP\n",
      ".....................................................................................................\n",
      "politics NNS\n",
      "situation NN\n",
      "uncle NN\n",
      "employee NN\n",
      "employers NNS\n",
      "hate NN\n",
      "speech NN\n",
      ".....................................................................................................\n",
      "kinda NN\n",
      "norm NN\n",
      "immigrants NNS\n",
      "illegal NN\n",
      "country NN\n",
      "work NN\n",
      "ass NN\n",
      "bullshit NN\n",
      "employers NNS\n",
      "workers NNS\n",
      ".....................................................................................................\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(comments_array)):\n",
    "    sentence_verification(comments_array[i])\n",
    "    print(\".....................................................................................................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will take only those words which are in all the three approaches and that will be our final set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank',\n",
       " 'speech',\n",
       " 'matter',\n",
       " 'form',\n",
       " 'illegal',\n",
       " 'world',\n",
       " 'People',\n",
       " 'hate',\n",
       " 'something',\n",
       " 'uncle',\n",
       " 'horse',\n",
       " 'needs',\n",
       " 'customers',\n",
       " 'bullshit',\n",
       " 'threatening',\n",
       " 'legend',\n",
       " 'days',\n",
       " 'custommer',\n",
       " 'people']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(arr1) & set(arr2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
